#!/bin/bash

usage() {
  echo "The goal of podman-ollama is to make AI even more boring."
  echo
  echo "Usage:"
  echo "  podman-ollama [prompt]"
  echo "  podman-ollama [options]"
  echo "  podman-ollama [command]"
  echo
  echo "Commands:"
  echo "  serve       Start ollama server (not required, other commands are serverless)"
  echo "  create      Create a model from a Modelfile"
  echo "  show        Show information for a model"
  echo "  run         Run a model, default if no command is specified"
  echo "  pull        Pull a model from a registry"
  echo "  push        Push a model to a registry"
  echo "  list        List models"
  echo "  cp          Copy a model"
  echo "  rm          Remove a model"
  echo "  help        Help about any command"
  echo "  generate    Generate structured data based on containers, pods or volumes"
  echo
  echo "Options:"
  echo "  -c, --container-manager CONMAN - Specify podman or docker, default: podman"
  echo "  -g, --gpu GPU                  - Specify a GPU: AMD, NVIDIA, GPU or CPU"
  echo "  -h, --help                     - Usage help"
  echo "  -l, --log LOGFILE              - Specify logfile to redirect to, for GPU debug"
  echo "  -m, --model MODEL              - Specify non-default model, default: mistral"
  echo "  -p, --publish                  - Publish a container's port to the host"
  echo "  -r, --root                     - Run as a rootful container"
  echo "  -v, --version                  - Show version information"
  echo "  -                              - Read from stdin"
  echo
  echo "Configuration:"
  echo "  podman-ollama uses a simple text format to store customizations that are are"
  echo "  per user in \"~/.podman-ollama/config\". Such a configuration file may look"
  echo "  like this:"
  echo
  echo "    container-manager podman"
  echo "    gpu GPU"
  echo "    model gemma:2b"
}

cleanup() {
  $SUDO $CON exec $CON_PS pkill ollama &
}

check_root() {
  if [ "$EUID" -eq 0 ]; then
    if ! $ROOT; then
      echo "to run as a rootful, insecure container, use this command:"
      echo "  podman-ollama -r"
      exit 3
    fi
  elif $ROOT; then
    if command -v sudo > /dev/null; then
      SUDO="sudo"
    else
      echo "to run as a rootful, insecure container, use this command as root user:"
      echo "  podman-ollama -r"
      exit 4
    fi
  fi
}

gpu_setup() {
  if [ -e "/dev/dri" ] && [ "$GPU" != "CPU" ]; then
    ADD="--device /dev/dri"
  fi

  POST="latest"
  if [ "$GPU" = "NVIDIA" ] || [ "$GPU" = "GPU" ] || [ "$GPU" = "CPU" ]; then
    POST="latest"
  elif [ "$GPU" = "AMD" ]; then
    ADD="$ADD --device /dev/kfd"
    POST="rocm"
  elif [ -e "/dev/kfd" ]; then
    for i in /sys/bus/pci/devices/*/mem_info_vram_total; do
      # AMD GPU needs more than 512M VRAM
      if [ "$(cat $i)" -gt "600000000" ]; then
        ADD="$ADD --device /dev/kfd"
        POST="rocm"
        break
      fi
    done
  fi
}

server_init() {
  check_root
  specify_gpu
  select_container_manager
  gpu_setup

  if [ "$OLLAMA_CMD" != "serve" ]; then
    trap cleanup EXIT
  fi

  if [ -n "$OLLAMA_FILE" ]; then
    ADD="$ADD -v"$OLLAMA_FILE":"$OLLAMA_FILE""
  fi

  VOL="-vollama:/root/.ollama"
  URL="docker.io/ollama/ollama:$POST"

  # This was introduced as part of ChromeOS support, but there was concerns
  # raised about rootful privileged containers, lets just do this in the
  # rootless case for now.
  if [ "$EUID" -eq 0 ]; then
    PRIV="--security-opt=label=disable"
  else # if rootless, just be --privileged
    PRIV="--privileged"
  fi

  if [ "$GPU" = "CPU" ]; then
    GPU=
  else
    GPU="--gpus=all"
    if [ $CON = "podman" ]; then
      if [ "$GPU" = "NVIDIA" ]; then
        GPU="$GPU --device nvidia.com/gpu=all"
      elif command -v nvidia-smi > /dev/null; then
        if [ "$GPU" = "GPU" ] || [ -z "$GPU" ]; then
          GPU="$GPU --device nvidia.com/gpu=all"
        fi
      fi
    fi
  fi

  if [ -n "$LOG" ]; then
    CON_PS="$(cat /proc/sys/kernel/random/uuid)"
    local name="--name $CON_PS"
    $SUDO $CON run --rm $PUBLISH $PRIV $name $ADD $GPU -v"$HOME":"$HOME" $VOL $URL > "$LOG" 2>&1 &
    while ! podman ps | grep -q $CON_PS; do
      sleep 0.01
    done
  else
    CON_PS=$($SUDO $CON run --rm $PUBLISH $PRIV $NAME $ADD $GPU -v"$HOME":"$HOME" $VOL -d $URL)
    RET="$?"
    if [ "$RET" -ne 0 ]; then
      echo "$CON_PS"
      exit $RET
    fi
  fi

  IS_SERVER_UP="false"
  for i in {1..16}; do
    if $SUDO $CON exec $CON_PS ollama ls > /dev/null 2>&1; then
      IS_SERVER_UP="true"
      break
    fi

    sleep 0.01
  done

  if ! $IS_SERVER_UP; then
    echo "Ollama service failed to be responsive"
    exit 5
  fi
}

select_container_manager() {
  if command -v podman > /dev/null; then
    CON="podman"
  elif command -v docker > /dev/null; then
    CON="docker"
  else
    CON="podman"
  fi

  if [ -n "$CONMAN" ]; then
    shopt -s nocasematch
    case "$CONMAN" in
      podman ) CON="podman";;
      docker) CON="docker";;
      *) echo "Unknown container manager: $CONMAN"; exit 2;;
    esac

    shopt -u nocasematch
  fi
}

specify_gpu() {
  if [ -n "$GPU" ]; then
    shopt -s nocasematch
    case "$GPU" in
      CPU ) GPU="CPU";;
      GPU ) GPU="GPU";;
      NVIDIA ) GPU="NVIDIA";;
      AMD) GPU="AMD";;
      *) echo "Unknown GPU: $GPU"; exit 2;;
    esac

    shopt -u nocasematch
  fi
}

set_default_vals() {
  LLM="mistral"
  STDIN="false"
  ROOT="false"
  GENERATE="false"
}

read_cfg_val() {
  local cfg_file="$HOME/.podman-ollama/config"
  sed -ne "s/^$1\s//pg" $cfg_file 2> /dev/null
}

set_config_vals() {
  local cfg_file="~/.podman-ollama/config"
  local conman=$(read_cfg_val "container-manager")
  if [ -n "$conman" ]; then
    CONMAN="$conman"
  fi

  local gpu=$(read_cfg_val "gpu")
  if [ -n "$gpu" ]; then
    GPU="$gpu"
  fi

  local log=$(read_cfg_val "log")
  if [ -n "$log" ]; then
    LOG="$log"
  fi

  local model=$(read_cfg_val "model")
  if [ -n "$model" ]; then
    LLM="$model"
  fi

  local publish=$(read_cfg_val "publish")
  if [ -n "$publish" ]; then
    PUBLISH="-p $publish"
  fi

  local root=$(read_cfg_val "root")
  if [ -n "$root" ]; then
    ROOT="true"
  fi
}

set -e -o pipefail

set_default_vals
set_config_vals

while [[ $# -gt 0 ]]; do
  case $1 in
    serve|create|show|run|pull|push|list|cp|rm|help)
      OLLAMA_CMD="$1"
      break
      ;;
    generate)
      PODMAN_CMD="$1"
      break
      ;;
    -c|--container-manager)
      CONMAN="$2"
      shift 2
      ;;
    -g|--gpu)
      GPU="$2"
      shift 2
      ;;
    -h|--help)
      usage
      exit 0
      ;;
    -l|--log)
      LOG="$2"
      shift 2
      ;;
    -m|--model)
      LLM="$2"
      shift 2
      ;;
    -p|--publish)
      PUBLISH="-p $2"
      shift 2
      ;;
    -r|--root)
      ROOT="true"
      shift 1
      ;;
    -v|--version)
      VERSION="$1"
      shift 1
      ;;
    -)
      STDIN="true"
      shift 1
      ;;
    -*|--*)
      usage
      echo "Unknown option $1"
      exit 1
      ;;
    *)
      break;
      ;;
  esac
done

if [ "$1" = "create" ]; then
  while [[ $# -gt 0 ]]; do
    case $3 in
        -f|--file)
          OLLAMA_MODEL="$2"
          OLLAMA_FILE="$(readlink -f $4)"
          shift 4
          break
          ;;
        *)
          break;
          ;;
      esac
  done

  if [ -z "$OLLAMA_FILE" ] && [ -e "Modelfile" ]; then
    OLLAMA_FILE="$(readlink -f Modelfile)"
    shift 1
  fi
fi

ARGV=("$@")
if [ "$1" = "generate" ]; then
  for (( j=0; j < $#; j++ )); do
    if [ "${ARGV[j]}" = "--name" ]; then
      j=$((j + 1))
      NAME="--name ${ARGV[j]}"
    fi
  done
fi

server_init

if [ "$1" = "serve" ]; then
  exit 0
fi

T=
if [ -t 1 ]; then
  T="-t"
fi

if [ "$1" = "generate" ]; then
  $SUDO $CON "$@"
elif [ -n "$VERSION" ]; then
  $SUDO $CON exec $T $CON_PS ollama $VERSION
elif [ -n "$OLLAMA_FILE" ]; then
  $SUDO $CON exec $T $CON_PS ollama create $OLLAMA_MODEL -f "$OLLAMA_FILE" $*
elif [ -n "$OLLAMA_CMD" ]; then
  $SUDO $CON exec $T -i $CON_PS ollama $*
elif $STDIN; then
  $SUDO $CON exec $T $CON_PS ollama run $LLM < /dev/stdin
elif [ -n "$1" ]; then
  $SUDO $CON exec $T $CON_PS ollama run $LLM "$1"
else
  $SUDO $CON exec $T -i $CON_PS ollama run $LLM
fi

